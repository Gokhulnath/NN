# -*- coding: utf-8 -*-
"""NN-HW5-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OdPNMnaT67u85T-Q9QKvWVp73iXF2gPg
"""

import numpy as np
import matplotlib.pyplot as plt

def gradient(w):
    w1, w2 = w
    grad_w1 = 26 * w1 - 10 * w2 + 4
    grad_w2 = -10 * w1 + 4 * w2 - 2
    return np.array([grad_w1, grad_w2])

def gradient_descent(eta, iterations, w_init):
    w = np.array(w_init)
    optimal_w = np.array([1, 3])
    distances = []

    for i in range(iterations):
        grad = gradient(w)
        w = w - eta * grad
        distance = np.linalg.norm(w - optimal_w)
        distances.append(distance)

    return distances

etas = [0.02, 0.05, 0.1]
iterations = 500
w_init = [0, 0]

for eta in etas:
    distances = gradient_descent(eta, iterations, w_init)
    plt.figure()
    plt.plot(distances, label=f'eta = {eta}')
    plt.xlabel('Iteration')
    plt.ylabel('Distance to Optimal Solution')
    plt.title(f'Gradient Descent: Distance vs Iteration for eta = {eta}')
    plt.legend()
    plt.show()