# -*- coding: utf-8 -*-
"""NN-HW5-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tYHgtobmS7GqLqaJqOxRXl3Op8wjbNmZ
"""

#old code from HW2 Q2

import numpy as np
import matplotlib.pyplot as plt

def step(v):
    return np.where(v >= 0, 1, 0)

W = np.array([[1, -1, 0], [-1, -1, -1]])
b = np.array([1, 1, -1])
U = np.array([1, 1, -1])
c = np.array([-1.5])

def neuralNetwork(x):
    hidden_layer = step(np.dot(x, W) + b)
    output = step(np.dot(hidden_layer, U) + c)
    return output

np.random.seed(102)
points = np.random.uniform(-2, 2, (1000, 2))
outputs = np.array([neuralNetwork(point) for point in points])

plt.scatter(points[:, 0], points[:, 1], c=outputs, cmap='coolwarm', s=15)
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()

# new Code for HW5 Q2

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(v, a=5):
    return 1 / (1 + np.exp(-a * v))

def sigmoid_derivative(v, a=5):
    sig = sigmoid(v, a)
    return a * sig * (1 - sig)

np.random.seed(102)
W = np.random.normal(0, 0.1, (2, 3))  # Shape of W is (2, 3)
b = np.random.normal(0, 0.1, (3,))    # Shape of b is (3,)
U = np.random.normal(0, 0.1, (3, 1))  # Shape of U is (3, 1) - for scalar output
c = np.random.normal(0, 0.1, (1,))    # Shape of c is (1,)

def forward_pass(x):
    v_z = np.dot(x, W) + b
    z = sigmoid(v_z)
    v_f = np.dot(z, U) + c
    f = sigmoid(v_f)
    return v_z, z, v_f, f

def backprop(x, y, v_z, z, v_f, f):
    delta_f = 2 * (f - y) * sigmoid_derivative(v_f)
    grad_U = z[:, np.newaxis] * delta_f
    grad_c = delta_f
    delta_z = (U @ delta_f) * sigmoid_derivative(v_z)
    grad_W = np.outer(x, delta_z)
    grad_b = delta_z

    return grad_W, grad_b, grad_U, grad_c

eta = 0.01
epochs = 100

np.random.seed(102)
points = np.random.uniform(-2, 2, (1000, 2))
true_outputs = np.array([neuralNetwork(point) for point in points])

mse_history = []
for epoch in range(epochs):
    mse = 0
    for x, y in zip(points, true_outputs):
        v_z, z, v_f, f = forward_pass(x)
        mse += (f - y)**2
        grad_W, grad_b, grad_U, grad_c = backprop(x, y, v_z, z, v_f, f)
        W -= eta * grad_W
        b -= eta * grad_b
        U -= eta * grad_U
        c -= eta * grad_c

    mse_history.append(mse / len(points))

plt.plot(mse_history)
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.title('MSE vs Epoch')
plt.show()

fig = plt.figure(figsize=(7, 10))
ax = plt.axes(projection="3d")
outputs = np.array([forward_pass(point)[-1] for point in points])

ax.scatter3D(points[:, 0], points[:, 1], outputs, color="green")
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f(x1, x2)')
plt.show()

# 2)F
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(v, a=5):
    return 1 / (1 + np.exp(-a * v))

def sigmoid_derivative(v, a=5):
    sig = sigmoid(v, a)
    return a * sig * (1 - sig)

np.random.seed(102)
W = np.random.normal(0, 0.1, (2, 3))  # Shape of W is (2, 3)
b = np.random.normal(0, 0.1, (3,))    # Shape of b is (3,)
U = np.random.normal(0, 0.1, (3, 1))  # Shape of U is (3, 1) - for scalar output
c = np.random.normal(0, 0.1, (1,))    # Shape of c is (1,)

def forward_pass(x):
    v_z = np.dot(x, W) + b
    z = sigmoid(v_z)
    v_f = np.dot(z, U) + c
    f = sigmoid(v_f)
    return v_z, z, v_f, f

def backprop(x, y, v_z, z, v_f, f):
    delta_f = 2 * (f - y) * sigmoid_derivative(v_f)
    grad_U = z[:, np.newaxis] * delta_f
    grad_c = delta_f

    delta_z = (U @ delta_f) * sigmoid_derivative(v_z)
    grad_W = np.outer(x, delta_z)
    grad_b = delta_z

    return grad_W, grad_b, grad_U, grad_c

initial_eta = 0.1
epochs = 100
batch_size = 32
T = 5

np.random.seed(102)
points = np.random.uniform(-2, 2, (1000, 2))
true_outputs = np.array([neuralNetwork(point) for point in points])

mse_history = []
current_eta = initial_eta

for epoch in range(epochs):
    mse = 0
    batch_count = len(points) // batch_size
    for i in range(batch_count):
        batch_indices = np.random.choice(len(points), batch_size)
        batch_x = points[batch_indices]
        batch_y = true_outputs[batch_indices]

        total_grad_W = np.zeros_like(W)
        total_grad_b = np.zeros_like(b)
        total_grad_U = np.zeros_like(U)
        total_grad_c = np.zeros_like(c)

        for x, y in zip(batch_x, batch_y):
            v_z, z, v_f, f = forward_pass(x)

            mse += (f - y)**2

            grad_W, grad_b, grad_U, grad_c = backprop(x, y, v_z, z, v_f, f)

            total_grad_W += grad_W
            total_grad_b += grad_b
            total_grad_U += grad_U
            total_grad_c += grad_c

        W -= current_eta * (total_grad_W / batch_size)
        b -= current_eta * (total_grad_b / batch_size)
        U -= current_eta * (total_grad_U / batch_size)
        c -= current_eta * (total_grad_c / batch_size)

    mse_history.append(mse / len(points))

    if epoch >= T and mse_history[-1] > mse_history[-T]:
        current_eta *= 0.9

plt.plot(mse_history)
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.title('MSE vs Epoch (with Hacks)')
plt.show()

fig = plt.figure(figsize=(7, 10))
ax = plt.axes(projection="3d")
outputs = np.array([forward_pass(point)[-1] for point in points])

ax.scatter3D(points[:, 0], points[:, 1], outputs, color="green")
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f(x1, x2)')
plt.show()